{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOysylRgwQQ+5gizBMm70Nh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izabellakacprzak/twitter-location-ner/blob/master/CustomNERModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A custom Twitter Location NER Model based on the token classification Bert model.\n",
        "Training data should be provided in **sentences.csv** in the following format:\n",
        "\n",
        "```\n",
        "Evansville #weather on November 13 2015 - 11/13/2015 http://t.co/l2Tc6nU3kI,I-LOC O O O O O O O O\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3u09EYIvHVB7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqBYSFMzmMYK"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "#             IMPORTS             #\n",
        "###################################\n",
        "\n",
        "!pip install transformers\n",
        "!pip install keras\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertModel, BertForTokenClassification\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AdamW\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#      EVALUATION FUNCTIONS       #\n",
        "###################################\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "# Computes a confusion matrix of the format above based on y_pred (predictions)\n",
        "#   and y_test (gold)\n",
        "# Note: y_pred and y_test should be numpy arrays\n",
        "def confusion_matrix(y_pred, y_test):\n",
        "  # For each prediction, convert to a tag of 0 or 1\n",
        "  y_pred_tag = np.round(sigmoid(y_pred)).astype('int32')\n",
        "  y_test = y_test.astype('int32')\n",
        "\n",
        "  # Since this is binary classification, the confusion matrix is (2, 2)\n",
        "  confusion = np.zeros((2, 2), dtype=np.int)\n",
        "\n",
        "  # Count each class for each prediction\n",
        "  for i in range(len(y_pred_tag)):\n",
        "    confusion[y_test[i], y_pred_tag[i]] += 1\n",
        "\n",
        "  return confusion\n",
        "\n",
        "# Calculates precision from a confusion matrix of the format above\n",
        "# Note: class_label should be CORRECT or INCORRECT\n",
        "def precision(confusion, class_label):\n",
        "  true_pos = confusion[1][class_label]\n",
        "  false_pos = confusion[0][class_label]\n",
        "\n",
        "  return true_pos / (true_pos + false_pos)\n",
        "\n",
        "# Calculates recall from s a confusion matrix of the format above\n",
        "# Note: class_label should be CORRECT or INCORRECT\n",
        "def recall(confusion, class_label):\n",
        "  true_pos = confusion[class_label][1]\n",
        "  false_neg = confusion[class_label][0]\n",
        "\n",
        "  return true_pos / (true_pos + false_neg)\n",
        "\n",
        "# Calculates f1 measure from a confusion matrix of the format above\n",
        "# Note: class_label should be CORRECT or INCORRECT\n",
        "def f_one_measure(confusion, class_label):\n",
        "  total_precision = precision(confusion, class_label)\n",
        "  total_recall = recall(confusion, class_label)\n",
        "\n",
        "  return (2 * total_precision * total_recall) / (total_precision + total_recall)\n",
        "\n",
        "# Calculates the average f1 measure from a confusion matrix of the format above\n",
        "def avg_f_one_measure(confusion):\n",
        "  correct_f_one = f_one_measure(confusion, 0)\n",
        "  incorrect_f_one = f_one_measure(confusion, 1)\n",
        "\n",
        "  return (correct_f_one + incorrect_f_one) / 2\n",
        "\n",
        "# Calculates the accuracy from a confusion matrix of the format above\n",
        "def accuracy(confusion):\n",
        "  true_pos = confusion[1][1]\n",
        "  true_neg = confusion[0][0]\n",
        "\n",
        "  total = sum([np.sum(row) for row in confusion])\n",
        "\n",
        "  return (true_pos + true_neg) / total"
      ],
      "metadata": {
        "id": "0CGlTYTXmiiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#          SET CONSTANTS          #\n",
        "###################################\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "LEARNING_RATE = 1e-7\n",
        "BATCH_SIZE = 10\n",
        "UNIQUE_LABELS = 2\n",
        "EPOCHS = 5\n",
        "DATA_FILE = \"sentences.csv\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "metadata": {
        "id": "TTA6fjEqmn54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#            TOKENIZER            #\n",
        "###################################\n",
        "\n",
        "def tokenize_and_preserve_labels(tokenizer, sentence, text_labels):\n",
        "  tokenized_sentence = []\n",
        "  labels = []\n",
        "\n",
        "  for word, label in zip(sentence, text_labels):\n",
        "      tokenized_word = tokenizer.tokenize(word)\n",
        "      n_subwords = len(tokenized_word)\n",
        "\n",
        "      # Add the tokenized word to the final tokenized word list\n",
        "      tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "      # Add the same label to the new list of labels `n_subwords` times\n",
        "      labels.extend([label] * n_subwords)\n",
        "\n",
        "  return tokenized_sentence, labels\n",
        "\n",
        "# A mapping of tags to indexes\n",
        "tags2index = {t:i for i,t in enumerate([\"O\", \"I-LOC\"])}\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "\n",
        "# Getting the pre-generated sentences\n",
        "df = pd.read_csv(DATA_FILE, delimiter=',', header=None, names=['texts', 'tags'], quoting=csv.QUOTE_NONE)\n",
        "# df = shuffle(df)\n",
        "\n",
        "sentences = [sent.split(\" \") for sent in df.texts.values]\n",
        "labels = [tags.split(\" \") for tags in df.tags.values]\n",
        "\n",
        "tokenized_texts_and_labels = [\n",
        "    tokenize_and_preserve_labels(tokenizer, sent, labs)\n",
        "    for sent, labs in zip(sentences, labels)\n",
        "]\n",
        "\n",
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
        "\n",
        "# Pad sentences and labels to MAX_SEQUENCE_LENGTH\n",
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAX_SEQUENCE_LENGTH, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "tags = pad_sequences([[tags2index.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAX_SEQUENCE_LENGTH, value=tags2index[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "\n",
        "# Split the dataset into train, validation and test\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
        "\n",
        "tr_inputs, rest_inputs, tr_tags, rest_tags = train_test_split(input_ids, tags,\n",
        "                                                            random_state=2018, test_size=0.2, train_size=0.8)\n",
        "tr_masks, rest_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.2, train_size=0.8)\n",
        "\n",
        "val_inputs, test_inputs, val_tags, test_tags = train_test_split(rest_inputs, rest_tags,\n",
        "                                                            random_state=2018, test_size=0.5, train_size=0.5)\n",
        "val_masks, test_masks, _, _ = train_test_split(rest_masks, rest_inputs,\n",
        "                                             random_state=2018, test_size=0.5, train_size=0.5)\n",
        "\n",
        "print(np.unique(tr_tags, return_counts=True))\n",
        "print(val_tags)\n",
        "# Creating tensors for training, validation and testing sentences, labels and attention masks\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "test_tags = torch.tensor(test_tags)\n",
        "\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "\n",
        "# Creating train, validation and test data loaders\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "validation_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_tags)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"Data Loaders preparation complete\")"
      ],
      "metadata": {
        "id": "zzDXq-Bemxiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################\n",
        "#       OPTIMISER AND LOSS        #\n",
        "###################################\n",
        "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=UNIQUE_LABELS,\n",
        "                                                                       output_attentions = False,\n",
        "                                                                       output_hidden_states = False)\n",
        "\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "PuGk0Raqn189"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#         TRAINING LOOP          #\n",
        "##################################\n",
        "model.cuda()\n",
        "\n",
        "# Store the current epoch number\n",
        "epochs = 0\n",
        "\n",
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "# Iterate for up to 25 epochs\n",
        "while epochs < EPOCHS:\n",
        "  epochs += 1\n",
        "  \n",
        "  # Tracking variables (nb = Naive Bayes, tr=Tracking)\n",
        "  tr_loss = 0\n",
        "  nb_tr_steps = 0\n",
        "\n",
        "  print(\"Processing Epoch Number: {}\".format(epochs))\n",
        "  \n",
        "\n",
        "  ## Training ##\n",
        "\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from the dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Clear out the gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    # Shape of outputs -> (batch_size, num_features)\n",
        "    #                     (so in this case 'torch.Size([32, 1])')\n",
        "    outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "    # Make b_labels the same shape as outputs and convert to float\n",
        "    #     (i.e. from 'torch.Size([32])' to 'torch.Size([32, 1])')\n",
        "    # b_labels = b_labels.unsqueeze(1)\n",
        "    b_labels = b_labels.float()\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = outputs[0]\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters and take a step\n",
        "    optimiser.step()\n",
        "\n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  # Total loss for this epoch\n",
        "  print(\" Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "\n",
        "  ## Validation ##\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  # Initialise confusion matrix\n",
        "  confusion = np.zeros((2, 2), dtype=np.int)\n",
        "\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  predictions , true_labels = [], []\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = [b.type(torch.LongTensor) for b in batch]\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Don't compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions (predicted values)\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "      \n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs[1].detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Calculate confusion matrix\n",
        "    eval_loss += outputs[0].mean().item()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    true_labels.extend(label_ids)\n",
        "\n",
        "  eval_loss = eval_loss / len(validation_dataloader)\n",
        "  validation_loss_values.append(eval_loss)\n",
        "  print(\"Validation loss: {}\".format(eval_loss))\n",
        "  preds = np.array([p_i for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if l_i != 2])\n",
        "  gold = np.array([p_i for p in true_labels for p_i in p if p_i != 2])\n",
        "  confusion = confusion_matrix(preds, gold)\n",
        "  print(\"Confusion matrix: {}\".format(confusion))\n",
        "  print(\"Accuracy: {}\".format(accuracy(confusion)))"
      ],
      "metadata": {
        "id": "LsPv1HtpnyYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################\n",
        "#         FINAL TESTING          #\n",
        "##################################\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialise confusion matrix\n",
        "confusion = np.zeros((UNIQUE_LABELS, UNIQUE_LABELS), dtype=np.int)\n",
        "\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "predictions , true_labels = [], []\n",
        "# Evaluate data for one epoch\n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = [b.type(torch.LongTensor) for b in batch]\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "  # Don't compute or store gradients\n",
        "  with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions (predicted values)\n",
        "    outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "      \n",
        "  # Move logits and labels to CPU\n",
        "  logits = outputs[1].detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Calculate confusion matrix\n",
        "  eval_loss += outputs[0].mean().item()\n",
        "  predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "  true_labels.extend(label_ids)\n",
        "\n",
        "eval_loss = eval_loss / len(validation_dataloader)\n",
        "validation_loss_values.append(eval_loss)\n",
        "print(\"Validation loss: {}\".format(eval_loss))\n",
        "preds = np.array([p_i for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if l_i != 2])\n",
        "gold = np.array([p_i for p in true_labels for p_i in p if p_i != 2])\n",
        "confusion = confusion_matrix(preds, gold)\n",
        "print(confusion)\n",
        "acc = accuracy(confusion)\n",
        "f1_zero = f_one_measure(confusion, 0)\n",
        "f1_location = f_one_measure(confusion, 1)\n",
        "print(\"F1 zero: {}\".format(f1_zero))\n",
        "print(\"F1 location: {}\".format(f1_location))\n",
        "f1 = avg_f_one_measure(confusion)\n",
        "print(\"Accuracy: {}\".format(acc))\n",
        "print(\"F1: {}\".format(f1))"
      ],
      "metadata": {
        "id": "8BrvxKXRn7Zc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}