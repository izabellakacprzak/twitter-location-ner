{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5YC8bSdI1YTNr0For7JRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izabellakacprzak/twitter-location-ner/blob/master/DataGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Generation tool for generating synthetic twitter data. Input data should be provided in **csvs/existing_sentences.csv**.\n",
        "\n",
        "Data can be generated in two different ways: using a Bert masked model or the Nominatim API. Result files are saved into **csvs/generated.csv**."
      ],
      "metadata": {
        "id": "6-gSSH_OIFEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#              CONSTANTS              #\n",
        "#######################################\n",
        "\n",
        "PREDICTIONS_LIMIT = 10\n",
        "SEARCH_LIMIT = 3\n",
        "NOMINATIM_API_URL = \"https://nominatim.openstreetmap.org\"\n",
        "NOMINATIM_SEARCH_ENDPOINT = f\"{NOMINATIM_API_URL}/search\""
      ],
      "metadata": {
        "id": "cIMh9XJn5ACF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#               IMPORTS               #\n",
        "#######################################\n",
        "\n",
        "!pip install transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "from transformers import BertTokenizer\n",
        "from pytorch_pretrained_bert import BertForMaskedLM\n",
        "import torch\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "import requests\n",
        "from typing import Dict\n",
        "import csv"
      ],
      "metadata": {
        "id": "zlRQAomwAAuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO9VG1eB3PU8"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "#   LOCATIONS GENERATION USING BERT   #\n",
        "#######################################\n",
        "\n",
        "# get PREDICTIONS_LIMIT number of predictions of masked words\n",
        "def get_predictions(original_sentence, masked_sentence):\n",
        "  text = '[MASK]'\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  mask_token = tokenizer.convert_tokens_to_ids(tokenized_text)[0]\n",
        "\n",
        "  # tokenize the text\n",
        "  tokenized_text = tokenizer.tokenize(masked_sentence)\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  # Create the segments tensors.\n",
        "  segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # Predict all tokens\n",
        "  with torch.no_grad():\n",
        "    predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "  # index of the masked token\n",
        "  mask_index = (tokens_tensor == mask_token).nonzero()[0][1].item()\n",
        "  # predicted token\n",
        "  predicted_index = torch.argmax(predictions[0, mask_index]).item()\n",
        "  max_indeces = []\n",
        "  ts = torch.argsort(predictions[0, mask_index], descending=True)[:PREDICTIONS_LIMIT]\n",
        "  for t in ts:\n",
        "    max_indeces.append(t.item())\n",
        "  predicted_tokens = tokenizer.convert_ids_to_tokens(max_indeces)\n",
        "    \n",
        "  return predicted_tokens\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-large-cased')\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#   SENTENCE GENERATION USING BERT    #\n",
        "#######################################\n",
        "\n",
        "# Generate new sentences based on input text\n",
        "# substituting existing locations with new\n",
        "# synthetically generated locations, generated using Bert.\n",
        "def generate_new_sentences(text, tags):\n",
        "  tokens = text.split()\n",
        "  labels = tags.split()\n",
        "  new_sentences = []\n",
        "  new_labels = []\n",
        "  for idx, token in enumerate(tokens):\n",
        "    # Only substitute existing locations\n",
        "    if labels[idx] == \"I-LOC\":\n",
        "      masked = tokens[:]\n",
        "      masked[idx] = \"[MASK]\"\n",
        "      masked = \"[CLS] \" + \" \".join(masked) + \" . [SEP]\"\n",
        "      locations = get_predictions(text, masked)\n",
        "      for location in locations:\n",
        "        doc = nlp(location)\n",
        "        if len(doc.ents) > 0 and doc.ents[0].label_ == \"GPE\":\n",
        "          new_sentence = tokens[:]\n",
        "          new_sentence[idx] = location\n",
        "          new_sentences.append(\" \".join(new_sentence))\n",
        "          new_labels.append(tags)\n",
        "  return new_sentences, new_labels"
      ],
      "metadata": {
        "id": "1GhLYzSC30uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "# SENTENCE GENERATION USING NOMINATIM #\n",
        "#######################################\n",
        "\n",
        "# Search Nominatim endpoint for query location\n",
        "def search_query(query: str, params: Dict[str, int]) -> dict:\n",
        "  if not(True in [char.isdigit() for char in query]):\n",
        "      params_query = \"&\".join(f\"{param_name}={param_value}\" for param_name, param_value in params.items())\n",
        "      request_url = f\"{NOMINATIM_SEARCH_ENDPOINT}?q={query}&{params_query}&format=json\"\n",
        "      print(request_url)\n",
        "\n",
        "      try:\n",
        "        response = requests.get(request_url)\n",
        "        response.raise_for_status()\n",
        "      except requests.exceptions.HTTPError as err:\n",
        "        return {}\n",
        "        \n",
        "      return response.json()\n",
        "    \n",
        "  return {}\n",
        "\n",
        "# Get new similar locations to input location\n",
        "def get_location_from_nominatim(location):\n",
        "  result = search_query(query=location.replace(\" \", \"+\").replace(\"#\", \"\"), params={})\n",
        "  return result\n",
        "\n",
        "# Returns a list of tokens generated from the input sentence\n",
        "# (a token is a non-location word or a location string)\n",
        "# and a list of ids of tokens which are locations\n",
        "def cluster_words_in_tokens(sent, labels):\n",
        "    location_idxs = []\n",
        "    tokens = []\n",
        "    words = sent.split()\n",
        "    labels = labels.split()\n",
        "    idx = 0\n",
        "    while idx < len(words):\n",
        "        token = []\n",
        "        if labels[idx] == \"I-LOC\":\n",
        "          token = []\n",
        "          while idx < len(words) and labels[idx] == \"I-LOC\":\n",
        "            location_idxs.append(idx)\n",
        "            token.append(words[idx])\n",
        "            idx += 1\n",
        "          tokens.append(\" \".join(token))\n",
        "        else:\n",
        "            tokens.append(words[idx])\n",
        "            idx += 1\n",
        "            \n",
        "    return tokens, location_idxs\n",
        "\n",
        "# Generate new sentences based on input text\n",
        "# substituting existing locations with new\n",
        "# synthetically generated locations, pulled from the Nominatim API.\n",
        "def generate_new_sentences_nominatim(text, tags):\n",
        "  if len(text.split()) != len(tags.split()):\n",
        "    print(\"bad stuff\")\n",
        "    return [], []\n",
        "  tokens, location_ids = cluster_words_in_tokens(text, tags)\n",
        "  tags = tags.split()\n",
        "  new_sentences = []\n",
        "  new_labels = []\n",
        "  for idx, token in enumerate(tokens):\n",
        "      if idx in location_ids:\n",
        "        locations = get_location_from_nominatim(token)\n",
        "        if locations != {}:\n",
        "          for location in locations[:SEARCH_LIMIT]:\n",
        "            new_sentence = tokens.copy()\n",
        "            new_sentence[idx] = location[\"display_name\"]\n",
        "            new_sentences.append(\" \".join(new_sentence))\n",
        "            new_tags = tags[:idx] + ([\"I-LOC\"] * len(new_sentence[idx].split())) + tags[idx+1:]\n",
        "            new_labels.append(\" \".join(new_tags))\n",
        "  return new_sentences, new_labels"
      ],
      "metadata": {
        "id": "u9cEsezS5T8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "#         SENTENCE GENERATION         #\n",
        "#######################################\n",
        "\n",
        "# For each sentence in csvs/existing_sentences.csv generate new sentences\n",
        "# with artificially computed new locations.\n",
        "# Generated sentences are saved in csvs/generated.csv\n",
        "# To switch between using Bert and the Nominatim API to generate\n",
        "# sentences set the \"source\" variable to \"BERT\" or \"NOMINATIM\" accordingly\n",
        "    \n",
        "source = \"NOMINATIM\"\n",
        "generated = open(\"csvs/generated.csv\", \"x\")\n",
        "generated_texts = []\n",
        "i = 0\n",
        "with open('csvs/existing_sentences.csv', 'r') as file:\n",
        "  reader = csv.reader(file)\n",
        "  for row in reader:\n",
        "      if i % 100 == 0:\n",
        "        print(i)\n",
        "      text = row[0]\n",
        "      labels = row[1]\n",
        "      if source == \"NOMINATIM\":\n",
        "        new_sentences, new_labels = generate_new_sentences_nominatim(text, labels)\n",
        "      else:\n",
        "        new_sentences, new_labels = generate_new_sentences(text, labels)\n",
        "      new_sentences.append(text)\n",
        "      new_labels.append(labels)\n",
        "            \n",
        "      for idx, sent in enumerate(new_sentences):\n",
        "          generated.write(sent)\n",
        "          generated.write(\",\")\n",
        "          generated.write(new_labels[idx])\n",
        "          generated.write(\"\\n\")\n",
        "      i += 1"
      ],
      "metadata": {
        "id": "Tcmt6QSS3cIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}